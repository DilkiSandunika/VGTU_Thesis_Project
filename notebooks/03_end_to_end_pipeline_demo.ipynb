{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOu8uo5iNSiPkRosEAvN7vb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4dee87fa814e4973a6432b380b2083b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_038c80fea0454877bd367464feb5034b",
              "IPY_MODEL_8c507d8b7abf461cb70ec52cfb7c9539",
              "IPY_MODEL_b91caeb65e224a9ea1610e0f1f3ef88b"
            ],
            "layout": "IPY_MODEL_e5130d37b8bb49dda01b9489eef153ea"
          }
        },
        "038c80fea0454877bd367464feb5034b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c261429991704b18a0513469a6d94da4",
            "placeholder": "​",
            "style": "IPY_MODEL_63737c6a64244bf7a0404cb4b7a828b1",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8c507d8b7abf461cb70ec52cfb7c9539": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbc60692a2dd40f5af0b78874bc627c1",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1a924a1b2fa54bf48323e7fda73baabb",
            "value": 2
          }
        },
        "b91caeb65e224a9ea1610e0f1f3ef88b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86ae5c35f10e4452b75d61d6873e18f5",
            "placeholder": "​",
            "style": "IPY_MODEL_93d84aeac38b4b9fbd9f59705e6c67b0",
            "value": " 2/2 [00:12&lt;00:00,  5.14s/it]"
          }
        },
        "e5130d37b8bb49dda01b9489eef153ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c261429991704b18a0513469a6d94da4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63737c6a64244bf7a0404cb4b7a828b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cbc60692a2dd40f5af0b78874bc627c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a924a1b2fa54bf48323e7fda73baabb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86ae5c35f10e4452b75d61d6873e18f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93d84aeac38b4b9fbd9f59705e6c67b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DilkiSandunika/VGTU_Thesis_Project/blob/main/notebooks/03_end_to_end_pipeline_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BXcNOWFD5sDK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4dee87fa814e4973a6432b380b2083b8",
            "038c80fea0454877bd367464feb5034b",
            "8c507d8b7abf461cb70ec52cfb7c9539",
            "b91caeb65e224a9ea1610e0f1f3ef88b",
            "e5130d37b8bb49dda01b9489eef153ea",
            "c261429991704b18a0513469a6d94da4",
            "63737c6a64244bf7a0404cb4b7a828b1",
            "cbc60692a2dd40f5af0b78874bc627c1",
            "1a924a1b2fa54bf48323e7fda73baabb",
            "86ae5c35f10e4452b75d61d6873e18f5",
            "93d84aeac38b4b9fbd9f59705e6c67b0"
          ]
        },
        "outputId": "f38ab0a3-de34-4806-f35d-58a8909e5ac7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required libraries for the full RAG pipeline with Gemma...\n",
            "Libraries installed successfully.\n",
            "Hugging Face token loaded successfully.\n",
            "Successfully logged in to Hugging Face.\n",
            "\n",
            "Loading all necessary components...\n",
            "Loaded 115 requirements from the CSV file.\n",
            "Loaded FAISS index and knowledge base documents.\n",
            "Sentence Transformer model loaded.\n",
            "\n",
            "Loading Google Gemma model... This will take a few minutes and use significant RAM.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4dee87fa814e4973a6432b380b2083b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Gemma model loaded successfully!\n",
            "\n",
            "--- Setup is complete and all components are ready! ---\n",
            "=====================================================================\n",
            "       RUNNING RAG PIPELINE DEMO on the First 5 Requirements      \n",
            "=====================================================================\n",
            "\n",
            "\n",
            "Processing Requirement #1...\n",
            "---------------------------------------------------------------------\n",
            "[INPUT] Original Requirement:\n",
            "'The solution should provide detailed context-sensitive help material for all the possible actions and scenarios on all user interfaces in the application.'\n",
            "\n",
            "[STEP 1 - RETRIEVAL] Finding the most relevant rules from the knowledge base...\n",
            "  - Found the following rules:\n",
            "    - All extracted functional requirements must strictly follow this format: \"The system shall [action description] for the [user role].\" The requirement must be a complete, standalone sentence. For example: \"The system shall generate a monthly report for the administrator.\"\n",
            "    - Rule 101: All functional requirements must explicitly state the user role involved (e.g., 'the admin', 'the user', 'the officer').\n",
            "    - Rule 105: Each requirement must be atomic, meaning it describes a single, verifiable function.\n",
            "\n",
            "[STEP 2 - GENERATION] Sending the original requirement and retrieved rules to Gemma for refinement...\n",
            "\n",
            "---------------------------------------------------------------------\n",
            "[OUTPUT] Final, Compliant Requirement:\n",
            "'-sensitive help material for all the possible actions and scenarios on all user interfaces in the application, ensuring that users can easily understand and utilize the application's functionalities.'\n",
            "=====================================================================\n",
            "\n",
            "\n",
            "Processing Requirement #2...\n",
            "---------------------------------------------------------------------\n",
            "[INPUT] Original Requirement:\n",
            "'The solution should provide detailed context-sensitive help material for all the possible actions and scenarios on all user interfaces in the application.'\n",
            "\n",
            "[STEP 1 - RETRIEVAL] Finding the most relevant rules from the knowledge base...\n",
            "  - Found the following rules:\n",
            "    - All extracted functional requirements must strictly follow this format: \"The system shall [action description] for the [user role].\" The requirement must be a complete, standalone sentence. For example: \"The system shall generate a monthly report for the administrator.\"\n",
            "    - Rule 101: All functional requirements must explicitly state the user role involved (e.g., 'the admin', 'the user', 'the officer').\n",
            "    - Rule 105: Each requirement must be atomic, meaning it describes a single, verifiable function.\n",
            "\n",
            "[STEP 2 - GENERATION] Sending the original requirement and retrieved rules to Gemma for refinement...\n",
            "\n",
            "---------------------------------------------------------------------\n",
            "[OUTPUT] Final, Compliant Requirement:\n",
            "'-sensitive help material for all the possible actions and scenarios on all user interfaces in the application, ensuring that users can easily understand and utilize the application's functionalities.'\n",
            "=====================================================================\n",
            "\n",
            "\n",
            "Processing Requirement #3...\n",
            "---------------------------------------------------------------------\n",
            "[INPUT] Original Requirement:\n",
            "'The solution should provide an interface for the user to log any defects or enhancement requests on the application and track thereafter.'\n",
            "\n",
            "[STEP 1 - RETRIEVAL] Finding the most relevant rules from the knowledge base...\n",
            "  - Found the following rules:\n",
            "    - All extracted functional requirements must strictly follow this format: \"The system shall [action description] for the [user role].\" The requirement must be a complete, standalone sentence. For example: \"The system shall generate a monthly report for the administrator.\"\n",
            "    - Rule 102: Any requirement handling personally identifiable information (PII) or sensitive data must mention encryption or secure handling.\n",
            "    - Rule 104: Requirements must be written in a clear, active voice (e.g., \"The system shall do X\" not \"X should be done\").\n",
            "\n",
            "[STEP 2 - GENERATION] Sending the original requirement and retrieved rules to Gemma for refinement...\n",
            "\n",
            "---------------------------------------------------------------------\n",
            "[OUTPUT] Final, Compliant Requirement:\n",
            "'The system shall allow users to submit and track defect reports, enabling them to provide feedback and suggest improvements for the application.\"'\n",
            "=====================================================================\n",
            "\n",
            "\n",
            "Processing Requirement #4...\n",
            "---------------------------------------------------------------------\n",
            "[INPUT] Original Requirement:\n",
            "'The solution should send alerts (e.g., email, SMS) to the user if the user chooses to whenever any action has been taken on the alert.'\n",
            "\n",
            "[STEP 1 - RETRIEVAL] Finding the most relevant rules from the knowledge base...\n",
            "  - Found the following rules:\n",
            "    - Rule 104: Requirements must be written in a clear, active voice (e.g., \"The system shall do X\" not \"X should be done\").\n",
            "    - All extracted functional requirements must strictly follow this format: \"The system shall [action description] for the [user role].\" The requirement must be a complete, standalone sentence. For example: \"The system shall generate a monthly report for the administrator.\"\n",
            "    - Rule 103: The system shall use role-based access control for any function that creates, modifies, or deletes data.\n",
            "\n",
            "[STEP 2 - GENERATION] Sending the original requirement and retrieved rules to Gemma for refinement...\n",
            "\n",
            "---------------------------------------------------------------------\n",
            "[OUTPUT] Final, Compliant Requirement:\n",
            "'The system shall send alerts (e.g., email, SMS) to the user when any action is taken on the alert, regardless of the user's role.\"'\n",
            "=====================================================================\n",
            "\n",
            "\n",
            "Processing Requirement #5...\n",
            "---------------------------------------------------------------------\n",
            "[INPUT] Original Requirement:\n",
            "'The solution should enable the user to track the submitted defect or enhancement request.'\n",
            "\n",
            "[STEP 1 - RETRIEVAL] Finding the most relevant rules from the knowledge base...\n",
            "  - Found the following rules:\n",
            "    - Rule 104: Requirements must be written in a clear, active voice (e.g., \"The system shall do X\" not \"X should be done\").\n",
            "    - All extracted functional requirements must strictly follow this format: \"The system shall [action description] for the [user role].\" The requirement must be a complete, standalone sentence. For example: \"The system shall generate a monthly report for the administrator.\"\n",
            "    - Rule 101: All functional requirements must explicitly state the user role involved (e.g., 'the admin', 'the user', 'the officer').\n",
            "\n",
            "[STEP 2 - GENERATION] Sending the original requirement and retrieved rules to Gemma for refinement...\n",
            "\n",
            "---------------------------------------------------------------------\n",
            "[OUTPUT] Final, Compliant Requirement:\n",
            "'The system shall generate a notification for the submitter when a new defect or enhancement request is submitted.\"'\n",
            "=====================================================================\n"
          ]
        }
      ],
      "source": [
        "# ===================================================================\n",
        "# CELL 1: Install All Necessary Libraries for Gemma\n",
        "# ===================================================================\n",
        "print(\"Installing required libraries for the full RAG pipeline with Gemma...\")\n",
        "# We need transformers and accelerate for Hugging Face models, and bitsandbytes for quantization\n",
        "!pip install pandas faiss-cpu sentence-transformers torch transformers accelerate bitsandbytes -q\n",
        "print(\"Libraries installed successfully.\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# CELL 2: Import Libraries and Log in to Hugging Face\n",
        "# ===================================================================\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pickle\n",
        "from google.colab import userdata\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# Securely load the Hugging Face token from Colab secrets\n",
        "try:\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    print(\"Hugging Face token loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(\"ERROR: Could not load Hugging Face token. Please add it to Colab's secrets (key icon on the left) with the name HF_TOKEN.\")\n",
        "\n",
        "# Log in to Hugging Face Hub\n",
        "from huggingface_hub import login\n",
        "login(token=HF_TOKEN)\n",
        "print(\"Successfully logged in to Hugging Face.\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# CELL 3: Load All Pre-processed Data and Models\n",
        "# ===================================================================\n",
        "print(\"\\nLoading all necessary components...\")\n",
        "\n",
        "# --- 1. Load the Parsed Requirements ---\n",
        "df_requirements = pd.read_csv('/content/parsed_requirements.csv')\n",
        "print(f\"Loaded {len(df_requirements)} requirements from the CSV file.\")\n",
        "\n",
        "# --- 2. Load the Knowledge Base ---\n",
        "index = faiss.read_index('/content/knowledge_base.index')\n",
        "with open('/content/knowledge_base_docs.pkl', 'rb') as f:\n",
        "    knowledge_base_docs = pickle.load(f)\n",
        "print(\"Loaded FAISS index and knowledge base documents.\")\n",
        "\n",
        "# --- 3. Load the Sentence Transformer Model ---\n",
        "retrieval_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"Sentence Transformer model loaded.\")\n",
        "\n",
        "# --- 4. Load the Gemma Model for Generation (This is the big step!) ---\n",
        "print(\"\\nLoading Google Gemma model... This will take a few minutes and use significant RAM.\")\n",
        "model_id = \"google/gemma-2b-it\"\n",
        "\n",
        "# Use quantization to make the model fit into Colab's free GPU memory\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "gemma_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\" # This automatically uses the GPU if available\n",
        ")\n",
        "print(\"Google Gemma model loaded successfully!\")\n",
        "print(\"\\n--- Setup is complete and all components are ready! ---\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# CELL 4: The RAG Core Functions (Updated for Gemma)\n",
        "# ===================================================================\n",
        "\n",
        "def retrieve_relevant_knowledge(query_text, top_k=3):\n",
        "    \"\"\"\n",
        "    Searches the FAISS index for the most relevant knowledge base documents for a given query.\n",
        "    \"\"\"\n",
        "    query_vector = retrieval_model.encode([query_text])\n",
        "    distances, indices = index.search(query_vector.astype('float32'), top_k)\n",
        "    retrieved_docs = [knowledge_base_docs[i] for i in indices[0]]\n",
        "    return retrieved_docs\n",
        "\n",
        "def generate_compliant_requirement_with_gemma(original_requirement, retrieved_docs):\n",
        "    \"\"\"\n",
        "    Builds a prompt and calls the Gemma model to generate a refined requirement.\n",
        "    \"\"\"\n",
        "    retrieved_knowledge = \"\\n- \".join(retrieved_docs)\n",
        "\n",
        "    # Gemma uses a specific chat template format. We must follow it precisely.\n",
        "    chat = [\n",
        "        { \"role\": \"user\", \"content\": f\"\"\"\n",
        "You are an expert Software Requirements Analyst. Your task is to refine a given software requirement to ensure it is compliant with a set of rules and well-formed according to a template.\n",
        "\n",
        "**Compliance Rules and Template Guide to Follow:**\n",
        "- {retrieved_knowledge}\n",
        "\n",
        "**Original Requirement to Refine:**\n",
        "\"{original_requirement}\"\n",
        "\n",
        "**Your Task:**\n",
        "Rewrite the original requirement to be fully compliant with the rules provided above.\n",
        "- Ensure the output strictly follows the format: \"The system shall [action description] for the [user role].\"\n",
        "- The final output must be a single, refined sentence and nothing else. Do not add any extra explanations.\n",
        "\"\"\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Apply the chat template and convert to tensor inputs\n",
        "    prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda\") # Send to GPU\n",
        "\n",
        "    # Generate the output\n",
        "    outputs = gemma_model.generate(input_ids=inputs, max_new_tokens=150)\n",
        "\n",
        "    # Decode and return the response\n",
        "    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # The response includes the original prompt, so we need to extract just the generated part\n",
        "    return response_text[len(prompt)-7:] # -7 is a small adjustment for the template tokens\n",
        "\n",
        "# ===================================================================\n",
        "# CELL 5: Run the End-to-End Demo on Multiple Requirements (Corrected)\n",
        "# ===================================================================\n",
        "\n",
        "# --- Configuration ---\n",
        "# Set the number of requirements you want to process from the start of the file.\n",
        "num_samples_to_process = 5\n",
        "\n",
        "print(\"=====================================================================\")\n",
        "print(f\"       RUNNING RAG PIPELINE DEMO on the First {num_samples_to_process} Requirements      \")\n",
        "print(\"=====================================================================\")\n",
        "\n",
        "# --- Loop through the first N samples of the DataFrame ---\n",
        "# We use 'idx' for the loop variable to avoid conflict with the 'index' FAISS object.\n",
        "for idx, row in df_requirements.head(num_samples_to_process).iterrows():\n",
        "\n",
        "    original_req_text = row['text']\n",
        "\n",
        "    print(f\"\\n\\nProcessing Requirement #{idx + 1}...\")\n",
        "    print(\"---------------------------------------------------------------------\")\n",
        "\n",
        "    # --- Step 1: The Original Requirement ---\n",
        "    print(f\"[INPUT] Original Requirement:\\n'{original_req_text}'\")\n",
        "\n",
        "    # --- Step 2: Retrieval ---\n",
        "    print(\"\\n[STEP 1 - RETRIEVAL] Finding the most relevant rules from the knowledge base...\")\n",
        "    relevant_rules = retrieve_relevant_knowledge(original_req_text)\n",
        "    print(\"  - Found the following rules:\")\n",
        "    for rule in relevant_rules:\n",
        "        print(f\"    - {rule}\")\n",
        "\n",
        "    # --- Step 3: Generation ---\n",
        "    print(\"\\n[STEP 2 - GENERATION] Sending the original requirement and retrieved rules to Gemma for refinement...\")\n",
        "    refined_requirement = generate_compliant_requirement_with_gemma(original_req_text, relevant_rules)\n",
        "\n",
        "    # --- Step 4: The Final Output ---\n",
        "    print(\"\\n---------------------------------------------------------------------\")\n",
        "    print(f\"[OUTPUT] Final, Compliant Requirement:\\n'{refined_requirement}'\")\n",
        "    print(\"=====================================================================\")"
      ]
    }
  ]
}